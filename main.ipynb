{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import get_embeddings, login_Hugging_for_tokenizer_model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
       "Token is valid (permission: fineGrained).\n",
       "Your token has been saved to C:\\Users\\Yang\\.cache\\huggingface\\token\n",
       "Login successful\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer, model = login_Hugging_for_tokenizer_model()\n",
    "\n",
    "def smiles_to_embeddings(smiles, tokenizer=tokenizer, model=model):\n",
    "    return get_embeddings(tokenizer, model, smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 384])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "smiles = \"NCCCC\"\n",
    "\n",
    "# Get the embeddings\n",
    "embedding = smiles_to_embeddings(smiles)\n",
    "\n",
    "# Print or use the embedding as needed\n",
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.24266241  0.30873424  0.04592807  0.25971904 -0.2327013   0.03468586\n",
       "   0.25395554 -0.09177838  0.18316105 -0.2616902  -0.2962829  -0.07542133\n",
       "   0.07019384 -0.09132029 -0.08019534 -0.00945247 -0.3274298  -0.32247984\n",
       "  -0.0842604   0.11031325  0.3336127  -0.00563066  0.01571712 -0.301354\n",
       "  -0.24270968 -0.20475972 -0.1421593   0.23083301 -0.03602657 -0.0107397\n",
       "   0.04983145  0.15676363  0.02868942  0.04773795  0.26598844  0.09794639\n",
       "   0.07370745  0.09912263  0.06953901 -0.17010204  0.04149458  0.20518687\n",
       "   0.06785604  0.06076902  0.0584873  -0.12422742 -0.02947365 -0.11834992\n",
       "  -0.10592988 -0.01037932 -0.03798876  0.23551685 -0.07751242 -0.24929836\n",
       "  -0.0463852   0.12985167 -0.22289795 -0.1335349   0.04594678 -0.28542638\n",
       "  -0.27348894 -0.10488877  0.14732462 -0.09155297 -0.03746787 -0.10174252\n",
       "  -0.21891917  0.20941682  0.28171122  0.06663831 -0.03003342 -0.15222865\n",
       "  -0.16352792 -0.05705269 -0.27509323 -0.2041312   0.08902555 -0.1255533\n",
       "   0.05932727  0.0767987  -0.10255975 -0.03723477 -0.12937555  0.01692869\n",
       "  -0.00539941 -0.25009435  0.02510847  0.05895372 -0.25482094 -0.09339128\n",
       "  -0.27840117 -0.13858443 -0.4009371   0.20642465  0.13075735 -0.01373899\n",
       "   0.13441254  0.09091736 -0.14095983 -0.09511276  0.2809045  -0.00518675\n",
       "   0.33080322  0.00255931 -0.21760595 -0.3595468   0.39237392 -0.30787665\n",
       "  -0.07771239  0.23645887  0.13866429 -0.03718095  0.08614959  0.03810123\n",
       "  -0.08979096 -0.16427682 -0.1967804  -0.3009087  -0.02014565  0.0019772\n",
       "  -0.12623298  0.09432606 -0.11325937  0.09453075  0.16040507 -0.04007294\n",
       "   0.03649777  0.29905125 -0.313578    0.05545095 -0.0725877  -0.23293877\n",
       "  -0.22796386 -0.04403894  0.03787829  0.0435896  -0.15800008 -0.13621134\n",
       "   0.0052616  -0.01996296 -0.26744246  0.28289333  0.04428596 -0.08361474\n",
       "   0.07624409 -0.18986212  0.23297952 -0.04549965  0.16784093 -0.04194364\n",
       "  -0.16251017  0.27874076 -0.08357327  0.32885033 -0.23083436 -0.27388027\n",
       "  -0.4041065  -0.5231609   0.05980998  0.11641771 -0.04209187 -0.09424396\n",
       "   0.12725481  0.00905448 -0.3141389   0.21343732 -0.18380082  0.10951142\n",
       "   0.24687997 -0.26338828  0.03455356 -0.12131927 -0.04987904 -0.39879277\n",
       "  -0.1168527  -0.19924739  0.39670926 -0.24653196  0.06239977  0.02269234\n",
       "   0.14034203 -0.05249348  0.19707452  0.09397961 -0.04336997 -0.1230163\n",
       "  -0.25499505  0.2485889   0.0545729  -0.17902215  0.37678295  0.21295798\n",
       "  -0.05057268 -0.07510088 -0.3024838   0.06010364 -0.38715735  0.15134318\n",
       "   0.02632646 -0.35356122  0.10662919 -0.17098579 -0.12485796  0.22951755\n",
       "  -0.31251806 -0.24708478 -0.17168711 -0.04200588 -0.0206303  -0.15414959\n",
       "  -0.09779567 -0.31763688  0.31467968 -0.15895605  0.00795054 -0.08275791\n",
       "  -0.20440832  0.1586488  -0.03459884 -0.15178765 -0.11915414  0.2591849\n",
       "  -0.35062557  0.21399099  0.02804505  0.00247342  0.04045144 -0.20599471\n",
       "  -0.21875541  0.04569521  0.08894521  0.13810219 -0.06842723  0.01499327\n",
       "  -0.07975852 -0.14391874  0.15946822 -0.01417867 -0.17373824 -0.33962616\n",
       "  -0.04300998 -0.03391615  0.24446583 -0.18217742 -0.19780543  0.15801819\n",
       "   0.14451167  0.4833798  -0.41452643  0.10335968 -0.53924674 -0.23414594\n",
       "  -0.24782091 -0.04063282  0.04976226  0.12740548 -0.09382933 -0.06360473\n",
       "  -0.07283764 -0.08533639  0.00918341 -0.25433236 -0.05886328  0.00218505\n",
       "   0.46016586 -0.08301543 -0.09845234  0.00534284 -0.04220358 -0.31125426\n",
       "  -0.10821047 -0.00255578  0.19072564  0.07620484 -0.04851258 -0.01644763\n",
       "   0.0672934   0.20646915  0.01167306 -0.24535026 -0.08994722  0.01495874\n",
       "   0.04762959  0.06539346 -0.419507   -0.00556673 -0.06068517 -0.04967204\n",
       "   0.26972884 -0.09410671 -0.13293315  0.08442165 -0.05639037 -0.04034473\n",
       "  -0.05205107 -0.05193435 -0.09258221 -0.23067066  0.14256296 -0.19476746\n",
       "  -0.06417504  0.16945317 -0.2589606  -0.26437742 -0.12183664 -0.09685507\n",
       "   0.3572638  -0.14396861 -0.33618382  0.14339542  0.07253835 -0.09638478\n",
       "  -0.03024558 -0.14817354  0.3604763   0.02970765  0.12553622 -0.12839879\n",
       "   0.23280722  0.19775397 -0.13992837 -0.03723091 -0.19374645 -0.39751208\n",
       "   0.11386293  0.05453196 -0.34145665 -0.15080959  0.00613372 -0.50244737\n",
       "   0.03698309  0.38427255 -0.243876   -0.23883188  0.14753881  0.00892586\n",
       "   0.12457351  0.04782462 -0.25150248 -0.04973697 -0.3185802   0.08054002\n",
       "  -0.3887927   0.08862647  0.09398679  0.09136295  0.00780874  0.0141723\n",
       "   0.0168496  -0.14416547 -0.2818454  -0.00952558 -0.17768675  0.21070132\n",
       "   0.02811794  0.09801494  0.33439428  0.05561441 -0.04855444  0.06988491\n",
       "   0.2056008   0.11183606 -0.01522544 -0.05587032 -0.3054818  -0.12080428\n",
       "   0.21301898 -0.25332183  0.16496882  0.06291489  0.17823315 -0.08260306\n",
       "   0.16939251 -0.69909644  0.10460448 -0.02955075  0.41392165  0.23328122\n",
       "   0.01946223 -0.17354107 -0.0679941   0.01249634 -0.2171652  -0.03446181]]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "def get_smiles_embedding(smiles, model_version='DeepChem/ChemBERTa-77M-MTR', max_length=128):\n",
    "    model = AutoModel.from_pretrained(pretrained_model_name_or_path=model_version, output_attentions=True) # Load ChemBERT model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_version) # Load ChemBERT tokenizer\n",
    "    # Build tokenizer with same padding as in training\n",
    "    tokens = tokenizer(\n",
    "                smiles,\n",
    "                max_length=max_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "    # Get the embeddings from CLS token\n",
    "    with torch.no_grad():\n",
    "        emb = model(\n",
    "            tokens[\"input_ids\"],\n",
    "            tokens[\"attention_mask\"]\n",
    "        )[\"last_hidden_state\"][:, 0, :].numpy()\n",
    "    \n",
    "    return emb\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
